p_metodo_df <- melt(p_metodo, varnames = c("Variable1", "Variable2"), value.name = "Valor")
# Crear la visualización
method_plot <- ggplot(data = p_metodo_df, aes(x = Variable1, y = Variable2, fill = Valor)) +
geom_tile(color = "black") +
geom_text(aes(label = Valor),
size = 4,
color = "black") +
scale_fill_manual(values = c("pearson" = "#e033ff", "spearman" = "#fdaf7a"," " = "#e9e9e9"),na.value = "#e0ffbf") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.line = element_line(color = "black"),
panel.background = element_rect(fill = "#f9f9f9", color = NA),
plot.background = element_rect(fill = "#f9f9f9", color = NA),
legend.position = "none") +
labs(title = "Métodos de Correlación Utilizados y sus P-Valores",
x = "",
y = "",
fill = "Método")
method_plot <- ggplotly(method_plot) %>%
layout(autosize = TRUE,
xaxis = list(autorange = TRUE))
method_plot
corrplot <- ggcorrplot(cor_matrix,
method = "circle",
type = "full",
lab = TRUE,
lab_size = 5) +
scale_fill_gradient2(high ="#6400e4",mid="#f9e9e9",low = "#ff0000") +
scale_size_continuous(range = c(8, 25)) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
panel.background = element_rect(fill = "#f9f9f9", color = NA),
plot.background = element_rect(fill = "#f9f9f9", color = NA),
axis.text = element_text(size = 2),
legend.position = "none")
ggplotly(corrplot)
df_limpio <- df %>% select(-pais, -año) %>%
drop_na() %>%
filter(inflacion < quantile(inflacion, .975))%>%
arrange(desc(inflacion))
head(df_limpio)
set.seed(2706)
df_limpio_rescalado <- df_limpio %>% select(-region) %>% scale()
head(df_limpio_rescalado)
fviz_nbclust(df_limpio_rescalado, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2) +
labs(subtitle = "El método del codo")
kmeans_result <- kmeans(df_limpio_rescalado, centers = 8, nstart = 30)
df_limpio$cluster <- kmeans_result$cluster
head(df_limpio)
# Dividir df_limpio en train y test
set.seed(2706)
train_index <- createDataPartition(1:nrow(df_limpio), p = .8, list = FALSE)
df_train <- df_limpio[train_index, ]
df_test <- df_limpio[-train_index, ]
# Contar valores
cat("Número de observaciones en el conjunto de entrenamiento:", nrow(df_train), "\n")
cat("Número de observaciones en el conjunto de prueba:", nrow(df_test), "\n")
# Resumen de los conjuntos de entrenamiento y prueba
summary(df_train)
summary(df_test)
# Ajuste manual del número óptimo de árboles (ntree)
df_rmse <- data.frame(ntree = integer(), rmse = numeric())
ntree_values <- seq(5, 150, by = 1)
set.seed(2706)
for (nt in ntree_values) {
rf_model_temp <- randomForest(inflacion ~ ingreso_fiscal + pib + pib_per_capita + cpi + region + cluster,
data = df_train,
ntree = nt,
mtry = 2,
nodesize = 5)
pred_temp <- predict(rf_model_temp, df_test)
rmse_temp <- sqrt(mean((df_test$inflacion - pred_temp)^2))
# Guardar el RMSE
df_rmse <- rbind(df_rmse, data.frame(ntree = nt, rmse = rmse_temp))
}
df_rmse <- df_rmse %>% arrange(rmse)
head(df_rmse, 5)
# Gráfico de MSE vs ntree
plot <- ggplot(df_rmse, aes(x = ntree, y = rmse)) +
geom_line(color = "#4400a4") +
geom_point(color = "#4400a4", size=0.6) +
geom_area(fill = "#4400a4", alpha = 0.2) +
coord_cartesian(ylim = c( min(df_rmse$rmse), max(df_rmse$rmse))) +
labs(title = "RMSE vs Número de Árboles",
x = "Número de Árboles",
y = "RMSE")
ggplotly(plot)
posibles_arboles <- df_rmse %>% head(5) %>% arrange(ntree)
arboles_optimos <- posibles_arboles[1, "ntree"]
cat("Número óptimo de árboles:", arboles_optimos, "\n")
set.seed(2706)
# Ajustar el modelo con el número óptimo de árboles
rf_model_opt <- randomForest(inflacion ~ ingreso_fiscal + pib + pib_per_capita + cpi + cluster + region,
data = df_train,
ntree = arboles_optimos,
mtry = 2,
nodesize = 5)
# Resumen del modelo
print(rf_model_opt)
# Predecir la inflación en el conjunto de prueba
predicciones_opt <- predict(rf_model_opt, df_test)
# Evaluar el rendimiento del modelo optimizado (ejemplo con RMSE)
rmse_opt <-sqrt(mean((df_test$inflacion - predicciones_opt)^2))
cat("RMSE del modelo optimizado:", rmse_opt, "\n")
# Crear un gráfico de dispersión de los valores reales vs. las predicciones
df_pred <- data.frame(real = df_test$inflacion, prediccion = predicciones_opt, error_porcentual = (df_test$inflacion - predicciones_opt) / df_test$inflacion * 100)
plot <- ggplot(df_pred, aes(x = real, y = prediccion)) +
geom_point(color = "#4400a4") +
geom_abline(intercept = 0, slope = 1, color = "red") +
ylim(0, max(df_pred$real)) +
xlim(0, max(df_pred$real)) +
labs(title = "Valores Reales vs. Predicciones",
x = "Real",
y = "Predicción")
ggplotly(plot)
# Countplot error porcentual
#df_pred$error_porcentual <- round(df_pred$error_porcentual, 2)
#df_pred$error_porcentual <- ifelse(df_pred$error_porcentual < 0, df_pred$error_porcentual * -1, df_pred$error_porcentual)
#df_pred$error_porcentual <- ifelse(df_pred$error_porcentual > 200, 200, df_pred$error_porcentual)
sourceCpp("cpp/redondear_columna.cpp")
sourceCpp("cpp/redondear_columna.cpp")
sourceCpp("cpp/redondear_columna.cpp")
sourceCpp("cpp/redondear_columna.cpp")
sourceCpp("cpp/redondear_columna.cpp")
df_pred$error_porcentual <- redondear_columna(df_pred$error_porcentual, 300)
plot <- ggplot(df_pred, aes(x = error_porcentual)) +
geom_density(fill = "#4400a4", alpha= 0.6) +
labs(title = "Distribución del Error Porcentual",
x = "Error Porcentual",
y = "Frecuencia")
ggplotly(plot)
library(tidyr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(RColorBrewer)
library(ggcorrplot)
library(readxl)
library(plotly)
library(scales)
library(reshape2)
library(corrplot)
library(stringdist)
library(randomForest)
library(caret)
library(factoextra)
library(Rcpp)
inflacion <- read_excel("datasets/imf-dm-export-inflacion.xlsx")
head(inflacion[0:2])
pib <- read_excel("datasets/imf-dm-export-pib.xls")
head(pib[0:2])
ingreso_fiscal <- read_excel("datasets/imf-dm-export-presion-fiscal.xls")
head(ingreso_fiscal[0:2])
pib_per_capita <- read_excel("datasets/imf-dm-export-pib-per-capita.xls")
head(pib_per_capita[0:2])
formatear_fmi <- function(df, valor) {
# Contar los datos nulos
df <- df %>% replace(df == "no data", NA)
print(paste("Total Nulos:", sum(is.na(df))))
# Eliminar los datos nulos
df <- df %>% drop_na()
# Seleccionar las columnas de los años
años <- colnames(df)[-1]
# Cambiar el nombre de la columna de país
colnames(df)[1] <- "pais"
# Convertir las columnas a valores numéricos
df <- df %>% mutate_at(vars(-pais), as.numeric)
# Transformar el DataFrame de formato ancho a formato largo
df <- df %>% pivot_longer(cols = all_of(años)
, names_to ="año",
values_to = valor) %>%
mutate(across(c(valor, año), as.numeric))
# Mostrar los primeros registros y el resumen de los datos
print("Primeros Registros:")
print(head(df))
print("Resumen de los Datos:")
print(summary(df))
return(df)
}
inflacion <- formatear_fmi(inflacion, "inflacion")
pib <- formatear_fmi(pib, "pib")
ingreso_fiscal <- formatear_fmi(ingreso_fiscal, "ingreso_fiscal")
pib_per_capita <- formatear_fmi(pib_per_capita, "pib_per_capita")
fmi <- pib %>%
full_join(ingreso_fiscal, by = c("pais", "año")) %>%
full_join(pib_per_capita, by = c("pais", "año")) %>%
full_join(inflacion, by = c("pais", "año"))
fmi <- fmi %>% filter(año >= 1995 & año <= 2023)
cpi <- read.csv("datasets/cpi/CPI-Archive-2011.csv")[c(1,3,4)]
colnames(cpi) <- c("pais","region","score")
for (año in 2010:1995) {
# Importar los datos del CPI
cpi_año <- read.csv(paste0("datasets/cpi/CPI-Archive-",año,".csv"))[c(1,3,4)]
# Cambiar el nombre de la columna de país
colnames(cpi_año)[1] <- "pais"
colnames(cpi_año)[2] <- "region"
# Unir los datos del CPI
cpi <- cpi %>%
left_join(cpi_año, by = c("pais","region"), suffix = c("", año))
}
colnames(cpi)[3] <- "score2011"
# Convertir las columnas a numérico por error al pivotar
cpi <- cpi %>% mutate(across(starts_with("score"), as.numeric))
cpi <- cpi %>% pivot_longer( cols = starts_with("score"), names_to = "año", names_prefix = "score", values_to = "cpi") %>%
mutate(año = as.numeric(año))
# Ajustar la escala
cpi$cpi <- cpi$cpi * 10
# Importar el resto de años
cpi_2012_2022 <- read_excel("datasets/cpi/CPI-Archive-2012-2023.xlsx")[c(1,3,4,5)]
# Cambiar el nombre de las columnas
colnames(cpi_2012_2022) <- c("pais","año","region","cpi")
# Pasar a numérico
cpi_2012_2022 <- cpi_2012_2022 %>% mutate(cpi = as.numeric(cpi),
año = as.numeric(año))
# Unir los datos del CPI verticalmente
cpi <- rbind(cpi, cpi_2012_2022)
cpi <- cpi %>% mutate(region = as.factor(region))
fmi$pais <- recode(fmi$pais,
"Korea, Republic of" = "Korea (South)",
"China, People's Republic of" = "China",
"Congo, Dem. Rep. of the" = "Congo",
"Taiwan Province of China" = "Taiwan",
"Australia and New Zealand" = "Australia",
"São Tomé and Príncipe" = "Sao Tome and Principe",
"Gambia, The" = "Gambia",
"Bahamas, The" = "Bahamas",
"Congo, Republic of" = "Congo Republic",
"Hong Kong SAR" = "Hong Kong")
# Calcula la distancia de similitud entre los nombres de los países
distancias <- stringdistmatrix(fmi$pais, cpi$pais, method = "jw")
indices <- apply(distancias, 1, which.min)
nombres_equivalentes <- cpi$pais[indices]
# Calcula el porcentaje de error estimado
porcentaje_error <- apply(distancias, 1, min)
porcentaje_error <- porcentaje_error * 100
# Añadir el porcentaje de error estimado al dataframe original
fmi <- fmi %>%
mutate(porcentaje_error = porcentaje_error,
nombre_eq = nombres_equivalentes)
fmi %>% select(pais, nombre_eq, porcentaje_error) %>%  group_by(pais) %>% slice(1) %>% arrange(desc(porcentaje_error))
fmi <- fmi %>% ungroup() %>% filter(porcentaje_error == 0)
fmi$pais <- fmi$nombre_eq
fmi <- fmi %>% select(-nombre_eq, -porcentaje_error) %>%
mutate(año = as.numeric(año))
df <- cpi %>%
full_join(fmi, by = c("pais", "año"))
df <- df %>% arrange(pais, año)
df <- df[rowSums(is.na(df)) < 4, ]
summary(df)
head(df)
# Pasar el dataframe a formato largo
df_long <- df %>%
select(-pais, -region) %>%
pivot_longer(cols = -año, names_to = "variable", values_to = "valor") %>%
group_by(año, variable) %>%
summarize(n_nulos = sum(is.na(valor)), .groups = "drop")
# Crear el gráfico con ggplot2
nulos_año_plot <- ggplot(df_long, aes(x = año, y = n_nulos, fill = variable )) +
geom_bar(stat = "identity", width = 0.9, color="#aaaaaa") +
facet_wrap(~variable, scales = "free") +
labs(
title = "Cantidad de Nulos por Año y Variable",
x = "Año",
y = "Cantidad de Nulos"
) +
theme(legend.position = "none",
panel.spacing.x = unit(2, "lines"),
panel.spacing.y = unit(4, "lines"))
ggplotly(nulos_año_plot)
df_avg <- df %>%
group_by(año) %>%
summarise(
cpi = mean(cpi, na.rm = TRUE),
inflacion = median(inflacion, na.rm = TRUE),
pib = mean(pib, na.rm = TRUE),
pib_per_capita = mean(pib_per_capita, na.rm = TRUE),
ingreso_fiscal = mean(ingreso_fiscal, na.rm = TRUE),
.groups = "drop"
)
df_avg_largo <- df_avg %>%
pivot_longer(cols = -año, names_to = "variable", values_to = "valor")
# Crear el gráfico
plot <- ggplot(df_avg_largo, aes(x = año, y = valor, color = variable)) +
geom_line() +
geom_point(size = 2) +
facet_wrap(~variable, scales = "free") +
labs(
title = "Evolución de las Variables Numéricas (1995-2022) ",
x = "Año",
y = ""
) +
theme(legend.position = "none",
panel.spacing.x = unit(2, "lines"),
panel.spacing.y = unit(4, "lines"))
# Convertir a objeto plotly
ggplotly(plot)
# Seleccionar las columnas de interés
df_correlacion <- df %>% select(-pais, -año, -region) %>%
mutate( inflacion = log2(inflacion),
pib = log2(pib),
pib_per_capita = log2(pib_per_capita),
ingreso_fiscal = log2(ingreso_fiscal)) %>%
mutate_all(~rescale(.))
# Transformar el DataFrame de formato ancho a formato largo
df_correlacion_largo <-  df_correlacion %>% gather(variable, valor)
# Crear un gráfico de densidad para cada variable
plot <- ggplot(df_correlacion_largo, aes(x = valor, fill = variable)) +
geom_density() +
facet_wrap(~variable, scales = "free") +
labs(x = "",
y = "Densidad") +
theme(legend.position = "none",
panel.spacing.x = unit(1, "lines"),
panel.spacing.y = unit(3, "lines"))
# Convertir a objeto plotly
ggplotly(plot)
df_correlacion <- df %>% select(-pais, -año, -region) %>%
filter(inflacion < 75) %>%
mutate(pib = rescale(log10(pib)),
pib_per_capita = rescale(log10(pib_per_capita))) %>%
mutate_all(~rescale(.))
df_correlacion_largo <- df_correlacion %>%
pivot_longer(cols = -inflacion, names_to = "variable", values_to = "valor")
plot <- ggplot(df_correlacion_largo, aes(x = inflacion, y = valor)) +
geom_bin2d(binwidth = c(0.015, 0.015)) +
scale_fill_gradient(low = "#ada1ff", high = "#df0000") +
labs(
x = "",
y = "") +
ylim(0, 1) +
xlim(0, 1) +
facet_wrap(~variable, scales = "free") +
labs(
title = "Scatterplot de Inflación vs Resto de Variables",
x = "",
y = "") +
theme(legend.position = "none",
panel.spacing.x = unit(2, "lines"),
panel.spacing.y = unit(4, "lines"))
# Convertir a objeto plotly
ggplotly(plot)
df_correlacion <- df %>% select(-pais, -año, -region)
# Crear matrices vacías para almacenar p-valores más bajos, métodos y coeficientes de correlación
n <- ncol(df_correlacion)
metodos <- c("spearman", "pearson")
p_metodo <- matrix(" ", n, n, dimnames = list(names(df_correlacion), names(df_correlacion)))
cor_matrix <- matrix(1, n, n, dimnames = list(names(df_correlacion), names(df_correlacion)))
# Iterar sobre las combinaciones de pares de columnas
for (j in 1:(n-1)) {
for (k in (j+1):n) {
var1 <- names(df_correlacion)[j]
var2 <- names(df_correlacion)[k]
p_valores <- sapply(metodos, function(metodo) cor.test(df_correlacion[[var1]], df_correlacion[[var2]], method = metodo, use ="complete.obs")$p.value)
# Identificar el p-valor más bajo y el método correspondiente
metodo_elegido <- metodos[which.min(p_valores)]
p_valor_minimo <- min(p_valores)
# Calcular el coeficiente de correlación usando el método elegido
correlacion <- cor(df[[var1]], df[[var2]], method = metodo_elegido, use ="complete.obs")
# Almacenar el p-valor más bajo, el método elegido y el coeficiente de correlación
p_metodo[var1, var2] <- metodo_elegido
p_metodo[var2, var1] <- format(p_valor_minimo, digits = 2, scientific = TRUE)
cor_matrix[var1, var2] <- correlacion
cor_matrix[var2, var1] <- correlacion
}
}
p_metodo_df <- melt(p_metodo, varnames = c("Variable1", "Variable2"), value.name = "Valor")
# Crear la visualización
method_plot <- ggplot(data = p_metodo_df, aes(x = Variable1, y = Variable2, fill = Valor)) +
geom_tile(color = "black") +
geom_text(aes(label = Valor),
size = 4,
color = "black") +
scale_fill_manual(values = c("pearson" = "#e033ff", "spearman" = "#fdaf7a"," " = "#e9e9e9"),na.value = "#e0ffbf") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.line = element_line(color = "black"),
panel.background = element_rect(fill = "#f9f9f9", color = NA),
plot.background = element_rect(fill = "#f9f9f9", color = NA),
legend.position = "none") +
labs(title = "Métodos de Correlación Utilizados y sus P-Valores",
x = "",
y = "",
fill = "Método")
method_plot <- ggplotly(method_plot) %>%
layout(autosize = TRUE,
xaxis = list(autorange = TRUE))
method_plot
corrplot <- ggcorrplot(cor_matrix,
method = "circle",
type = "full",
lab = TRUE,
lab_size = 5) +
scale_fill_gradient2(high ="#6400e4",mid="#f9e9e9",low = "#ff0000") +
scale_size_continuous(range = c(8, 25)) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
panel.background = element_rect(fill = "#f9f9f9", color = NA),
plot.background = element_rect(fill = "#f9f9f9", color = NA),
axis.text = element_text(size = 2),
legend.position = "none")
ggplotly(corrplot)
df_limpio <- df %>% select(-pais, -año) %>%
drop_na() %>%
filter(inflacion < quantile(inflacion, .975))%>%
arrange(desc(inflacion))
head(df_limpio)
set.seed(2706)
df_limpio_rescalado <- df_limpio %>% select(-region) %>% scale()
head(df_limpio_rescalado)
fviz_nbclust(df_limpio_rescalado, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2) +
labs(subtitle = "El método del codo")
kmeans_result <- kmeans(df_limpio_rescalado, centers = 8, nstart = 30)
df_limpio$cluster <- kmeans_result$cluster
head(df_limpio)
# Dividir df_limpio en train y test
set.seed(2706)
train_index <- createDataPartition(1:nrow(df_limpio), p = .8, list = FALSE)
df_train <- df_limpio[train_index, ]
df_test <- df_limpio[-train_index, ]
# Contar valores
cat("Número de observaciones en el conjunto de entrenamiento:", nrow(df_train), "\n")
cat("Número de observaciones en el conjunto de prueba:", nrow(df_test), "\n")
# Resumen de los conjuntos de entrenamiento y prueba
summary(df_train)
summary(df_test)
# Ajuste manual del número óptimo de árboles (ntree)
df_rmse <- data.frame(ntree = integer(), rmse = numeric())
ntree_values <- seq(5, 150, by = 1)
set.seed(2706)
for (nt in ntree_values) {
rf_model_temp <- randomForest(inflacion ~ ingreso_fiscal + pib + pib_per_capita + cpi + region + cluster,
data = df_train,
ntree = nt,
mtry = 2,
nodesize = 5)
pred_temp <- predict(rf_model_temp, df_test)
rmse_temp <- sqrt(mean((df_test$inflacion - pred_temp)^2))
# Guardar el RMSE
df_rmse <- rbind(df_rmse, data.frame(ntree = nt, rmse = rmse_temp))
}
df_rmse <- df_rmse %>% arrange(rmse)
head(df_rmse, 5)
# Gráfico de MSE vs ntree
plot <- ggplot(df_rmse, aes(x = ntree, y = rmse)) +
geom_line(color = "#4400a4") +
geom_point(color = "#4400a4", size=0.6) +
geom_area(fill = "#4400a4", alpha = 0.2) +
coord_cartesian(ylim = c( min(df_rmse$rmse), max(df_rmse$rmse))) +
labs(title = "RMSE vs Número de Árboles",
x = "Número de Árboles",
y = "RMSE")
ggplotly(plot)
posibles_arboles <- df_rmse %>% head(5) %>% arrange(ntree)
arboles_optimos <- posibles_arboles[1, "ntree"]
cat("Número óptimo de árboles:", arboles_optimos, "\n")
set.seed(2706)
# Ajustar el modelo con el número óptimo de árboles
rf_model_opt <- randomForest(inflacion ~ ingreso_fiscal + pib + pib_per_capita + cpi + cluster + region,
data = df_train,
ntree = arboles_optimos,
mtry = 2,
nodesize = 5)
# Resumen del modelo
print(rf_model_opt)
# Predecir la inflación en el conjunto de prueba
predicciones_opt <- predict(rf_model_opt, df_test)
# Evaluar el rendimiento del modelo optimizado (ejemplo con RMSE)
rmse_opt <-sqrt(mean((df_test$inflacion - predicciones_opt)^2))
cat("RMSE del modelo optimizado:", rmse_opt, "\n")
# Crear un gráfico de dispersión de los valores reales vs. las predicciones
df_pred <- data.frame(real = df_test$inflacion, prediccion = predicciones_opt, error_porcentual = (df_test$inflacion - predicciones_opt) / df_test$inflacion * 100)
plot <- ggplot(df_pred, aes(x = real, y = prediccion)) +
geom_point(color = "#4400a4") +
geom_abline(intercept = 0, slope = 1, color = "red") +
ylim(0, max(df_pred$real)) +
xlim(0, max(df_pred$real)) +
labs(title = "Valores Reales vs. Predicciones",
x = "Real",
y = "Predicción")
ggplotly(plot)
# Countplot error porcentual
#df_pred$error_porcentual <- round(df_pred$error_porcentual, 2)
#df_pred$error_porcentual <- ifelse(df_pred$error_porcentual < 0, df_pred$error_porcentual * -1, df_pred$error_porcentual)
#df_pred$error_porcentual <- ifelse(df_pred$error_porcentual > 200, 200, df_pred$error_porcentual)
sourceCpp("cpp/redondear_columna.cpp")
df_pred$error_porcentual <- redondear_columna(df_pred$error_porcentual, 300)
plot <- ggplot(df_pred, aes(x = error_porcentual)) +
geom_density(fill = "#4400a4", alpha= 0.6) +
labs(title = "Distribución del Error Porcentual",
x = "Error Porcentual",
y = "Frecuencia")
ggplotly(plot)
# Importancia de las variables usando IncNodePurity
importancia <- importance(rf_model_opt)
importancia_df <- data.frame(variable = rownames(importancia),
importancia = importancia[, "IncNodePurity"])
# Ordena las variables por importancia
importancia_df <- importancia_df[order(importancia_df$importancia, decreasing = TRUE),]
plot <- ggplot(importancia_df, aes(x = reorder(variable, importancia), y = importancia, fill=variable)) +
geom_bar(stat = "identity" ) +
coord_flip() +
scale_fill_manual(values = c("#339aff", "#184d80", "#1e63a7", "#133d66", "#297bcb","#0f3051")) +
labs(title = "Importancia de las Variables",
x = "",
y = "Incremento en Pureza del Nodo") +
theme(legend.position = "none")
plot <- ggplotly(plot) %>% layout(xaxis = list(range = c(0, 4500)))
plot
plot <- ggplot(df_pred, aes(x = error_porcentual)) +
geom_histogram(fill = "#4400a4",bins = 50, alpha= 0.6) +
labs(title = "Distribución del Error Porcentual",
x = "Error Porcentual",
y = "Frecuencia")
ggplotly(plot)
